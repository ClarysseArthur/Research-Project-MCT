{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from Intersection import Lane, Approach, Intersection, Exit, Trafficlight\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit N\n",
    "exit_n_r = Lane([0], None, True, None)\n",
    "exit_n = Exit(0, [exit_n_r])\n",
    "\n",
    "# Exit E\n",
    "exit_e_r = Lane([0], None, True, None)\n",
    "exit_e_l = Lane([0], None, True, None)\n",
    "exit_e = Exit(1, [exit_e_r, exit_e_l])\n",
    "\n",
    "# Exit S\n",
    "exit_s_r = Lane([0], None, True, None)\n",
    "exit_s = Exit(2, [exit_s_r])\n",
    "\n",
    "# Exit W\n",
    "exit_w_r = Lane([0], None, True, None)\n",
    "exit_w_l = Lane([0], None, True, None)\n",
    "exit_w = Exit(3, [exit_w_r, exit_w_l])\n",
    "\n",
    "# Approach N\n",
    "traffic_light_n_r = Trafficlight(0, [1], 0, 1, 2)\n",
    "traffic_light_n_l = Trafficlight(1, [0, 2], 0, 1, 2)\n",
    "\n",
    "lane_n_r = Lane([1], traffic_light_n_r, False, [exit_w_r])\n",
    "lane_n_l = Lane([0, 2], traffic_light_n_l, False, [exit_s_r, exit_e_l])\n",
    "\n",
    "approach_n = Approach(0, [lane_n_r, lane_n_l], 90)\n",
    "\n",
    "# Approach E\n",
    "traffic_light_e_r = Trafficlight(2, [0, 1], 0, 1, 2)\n",
    "traffic_light_e_m = Trafficlight(3, [0], 0, 1, 2)\n",
    "traffic_light_e_l = Trafficlight(4, [2], 0, 1, 2)\n",
    "\n",
    "lane_e_r = Lane([0, 1], traffic_light_e_r, False, [exit_n_r, exit_w_r])\n",
    "lane_e_m = Lane([0], traffic_light_e_m, False, [exit_w_l])\n",
    "lane_e_l = Lane([2], traffic_light_e_l, False, [exit_s_r])\n",
    "\n",
    "approach_e = Approach(1, [lane_e_r, lane_e_m, lane_e_l], 0)\n",
    "\n",
    "# Approach S\n",
    "traffic_light_s_r = Trafficlight(5, [0, 1], 0, 1, 2)\n",
    "traffic_light_s_l = Trafficlight(6, [2], 0, 1, 2)\n",
    "\n",
    "lane_s_r = Lane([0, 1], traffic_light_s_r, False, [exit_e_r, exit_n_r])\n",
    "lane_s_l = Lane([2], traffic_light_s_l, False, [exit_w_l])\n",
    "\n",
    "approach_s = Approach(2, [lane_s_r, lane_s_l], 270)\n",
    "\n",
    "# Approach W\n",
    "traffic_light_w_r = Trafficlight(7, [0, 1], 0, 1, 2)\n",
    "traffic_light_w_m = Trafficlight(8, [0], 0, 1, 2)\n",
    "traffic_light_w_l = Trafficlight(9, [2], 0, 1, 2)\n",
    "\n",
    "lane_w_r = Lane([0, 1], traffic_light_w_r, False, [exit_s_r, exit_e_r])\n",
    "lane_w_m = Lane([0], traffic_light_w_m, False, [exit_e_l])\n",
    "lane_w_l = Lane([2], traffic_light_w_l, False, [exit_n_r])\n",
    "\n",
    "approach_w = Approach(3, [lane_w_r, lane_w_m, lane_w_l], 180)\n",
    "\n",
    "u =  [8, 8, 9, 9, 9, 8, 8, 9, 9, 9]\n",
    "i =  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
    "\n",
    "V = [[1, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "     [0, 0, 0, 1, 1, 0, 0, 0, 1, 1],\n",
    "     [0, 0, 1, 0, 0, 0, 0, 1, 0, 0]]\n",
    "\n",
    "m = len(V)\n",
    "a_max = 5\n",
    "\n",
    "intersection = Intersection([approach_n, approach_e, approach_s, approach_w], [exit_n, exit_e, exit_s, exit_w], u, i, V, a_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "### Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.70\n",
    "REPLAY_MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 64\n",
    "UPDATE_TARGET_INTERVAL = 50\n",
    "EPSILON = 0.95\n",
    "MIN_EPSILON = 0.01\n",
    "DECAY = 0.9999\n",
    "NR_EPISODES = 30\n",
    "\n",
    "state = intersection.reset()\n",
    "done =  False\n",
    "\n",
    "action_space = itertools.product(np.arange(0, a_max), repeat=m)\n",
    "action_space = np.array(list(action_space))\n",
    "action_space = action_space[~np.sometrue(action_space == 0, axis=1)]\n",
    "\n",
    "POSSIBLE_ACTIONS = np.arange(0, len(action_space)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent:\n",
    "    def __init__(self, replayCapacity, action_space):\n",
    "        ## Initialize replay memory\n",
    "        self.capacity = replayCapacity\n",
    "        self.memory = collections.deque(maxlen=self.capacity)\n",
    "        self.populated = False\n",
    "        ## q network\n",
    "\n",
    "\n",
    "        self.state_shape = [10]\n",
    "        self.action_space = np.array(action_space)\n",
    "        self.action_shape = len(self.action_space)\n",
    "\n",
    "        self.q_model = self.buildNetwork()\n",
    "        ## Target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        self.target_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "    def addToReplayMemory(self, step):\n",
    "        self.step = step\n",
    "        self.memory.append(self.step)\n",
    "\n",
    "    def sampleFromReplayMemory(self, batchSize):\n",
    "        self.batchSize = batchSize\n",
    "        if self.batchSize > len(self.memory):\n",
    "            self.populated = False\n",
    "            return self.populated\n",
    "        else:\n",
    "            return random.sample(self.memory, self.batchSize)\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_shape=self.state_shape, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "        model.add(Dense(self.action_shape, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr = 0.001), metrics=['MeanSquaredError'])\n",
    "        return model\n",
    "\n",
    "    def q_network_fit(self,batch, batchSize):\n",
    "        self.batchSize = batchSize\n",
    "        self.batch = batch\n",
    "\n",
    "    def q_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qPolicy = self.q_model.predict(self.state)\n",
    "        return self.qPolicy\n",
    "\n",
    "    def target_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qTarget = self.target_model.predict(self.state)\n",
    "        return self.qTarget\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.q_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQAgent(replayCapacity= REPLAY_MEMORY_CAPACITY, action_space=action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateCounter = 0\n",
    "rewardHistory = []\n",
    "\n",
    "action_space = itertools.product(np.arange(0, a_max), repeat=m)\n",
    "action_space = np.array(list(action_space))\n",
    "action_space = action_space[~np.sometrue(action_space == 0, axis=1)]\n",
    "\n",
    "\n",
    "for episode in range(NR_EPISODES):\n",
    "    episodeReward = 0\n",
    "    stepCounter = 0  # count the number of successful steps within the episode\n",
    "\n",
    "    #print('\\n', episode)\n",
    "    state = intersection.reset()\n",
    "    done = False\n",
    "    #state = np.expand_dims(state, axis=0)\n",
    "\n",
    "    while not done :\n",
    "        r = random.random()\n",
    "\n",
    "        if r <= EPSILON:\n",
    "            action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
    "            print(action)\n",
    "            #print('exploration')\n",
    "        else:\n",
    "            #print('exploitation')\n",
    "            qValues = agent.q_network_predict(state.reshape(1,-1))\n",
    "            action = np.argmax(qValues[0])\n",
    "            #print('action =', action)\n",
    "            #print(qValues)\n",
    "\n",
    "        newState, reward, done, info = intersection.step(action_space[action])\n",
    "\n",
    "        stepCounter +=1\n",
    "        #print('stepcounter = ', stepCounter)\n",
    "\n",
    "        #newState = np.expand_dims(newState, axis=0)\n",
    "        # store step in replay memory\n",
    "        step = (state, action, reward, newState, done)\n",
    "        agent.addToReplayMemory(step)\n",
    "        state = newState\n",
    "        episodeReward += reward\n",
    "        #print('episodeReward = ',episodeReward)\n",
    "        # When enough steps in replay memory -> train policy network\n",
    "        if len(agent.memory) >= (BATCH_SIZE ):\n",
    "            EPSILON = DECAY * EPSILON\n",
    "            print(EPSILON)\n",
    "            if EPSILON < MIN_EPSILON:\n",
    "                EPSILON = MIN_EPSILON\n",
    "            # sample minibatch from replay memory\n",
    "            miniBatch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
    "            miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
    "            miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
    "            miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
    "            miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
    "            miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
    "\n",
    "            current_state_q_values = agent.q_network_predict(miniBatch_states)\n",
    "            y = current_state_q_values\n",
    "            #print(y.shape)\n",
    "            #miniBatch_next_state = np.squeeze(miniBatch_next_state, axis =1)\n",
    "\n",
    "            next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
    "            #print(next_state_q_values.shape)\n",
    "            max_q_next_state = np.max(next_state_q_values,axis=1)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                if miniBatch_done[i]:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
    "                else:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + DISCOUNT * max_q_next_state[i]\n",
    "\n",
    "            agent.q_model.fit(miniBatch_states, y, batch_size=BATCH_SIZE, verbose = 0)\n",
    "            #print(y)\n",
    "\n",
    "        else:\n",
    "            # intersection.render()\n",
    "            continue\n",
    "        if updateCounter == UPDATE_TARGET_INTERVAL:\n",
    "            agent.update_target_network()\n",
    "            print('target updated')\n",
    "            updateCounter = 0\n",
    "        updateCounter += 1\n",
    "    print('episodeReward for episode ', episode, '= ', episodeReward, 'with epsilon = ', EPSILON)\n",
    "    rewardHistory.append(episodeReward)\n",
    "\n",
    "intersection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewardHistory)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(intersection.drukte_hist)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Aantal autos')\n",
    "plt.title('Drukte - DQN')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ResearchProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50d2a1ec1360ec825cb4fbe595e07b628a7831ef8bc05b08c7b94e28ba152b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
